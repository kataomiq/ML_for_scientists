# -*- coding: utf-8 -*-
"""Untitled25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IaJlweK6Jk6VcqXw3IZdzfRW3En5Wvwx
"""

import json
import re
import pandas as pd

class LatexTokenizer:
    def __init__(self, json_path='dictionary.json'):
        """
        Инициализация токенизатора
        :param json_path: Путь к JSON-словарю
        """
        # Загрузка данных словаря
        with open(json_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

        # Построение отображений
        self._build_mappings()

    def _build_mappings(self):
        """Построение внутренних отображений"""
        self.latex_category_map = {}
        for category in ['symbols', 'constant', 'operations', 'identifier']:
            for entry in self.data[category]:
                self.latex_category_map[entry['latex']] = category

        # Отображение символов на категории
        self.char_category_map = {}
        for category in ['symbols', 'constant', 'operations', 'identifier']:
            for entry in self.data[category]:
                self.char_category_map[entry['char']] = category

        # Множество цифровых символов
        self.number_chars = {entry['char'] for entry in self.data['constant'] if entry['char'].isdigit()}
        self.var_chars = {entry['char'] for entry in self.data['symbols']}

        # Список команд LaTeX, отсортированных по длине (сначала длинные)
        self.latex_list = sorted(
            self.latex_category_map.keys(),
            key=lambda x: (-len(x), x)
        )

        # Присвоение идентификаторов токенам
        self.latex_to_id = {latex: idx for idx, latex in enumerate(self.latex_category_map.keys())}
        self.id_to_latex = {v: k for k, v in self.latex_to_id.items()}

    def tokenize(self, s):
        """
        Метод токенизации
        :param s: Входная строка
        :return: Список токенов
        """
        # Шаг 1: разделение по пробелам и удаление пустых строк
        parts = [p for p in re.split(r'\s+', s) if p]
        tokens = []

        for part in parts:
            # Шаг 2: токенизация каждой части по правилам LaTeX
            i = 0
            while i < len(part):
                matched = False
                for latex in self.latex_list:
                    if part.startswith(latex, i):
                        tokens.append(latex)
                        i += len(latex)
                        matched = True
                        break
                if not matched:
                    tokens.append(part[i])
                    i += 1
        return tokens

    def encode(self, s):
        """
        Полный процесс кодирования
        :param s: Входная строка
        :return: Список ID токенов
        """
        tokens = self.tokenize(s)
        return self._encode_tokens(tokens)

    def _encode_tokens(self, tokens):
        """Внутренний метод кодирования"""
        encoded = []
        for token in tokens:
            # Случай 1: известная команда LaTeX
            if token in self.latex_category_map:
                category = self.latex_category_map[token]
                if category == 'constant':
                    encoded.append('<NUM>')
                elif category == 'symbols':
                    encoded.append('<VAR>')
                else:
                    encoded.append(str(self.latex_to_id[token]))

            # Случай 2: одиночный символ
            else:
                if token in self.number_chars:
                    encoded.append('<NUM>')
                elif token in self.var_chars:
                    encoded.append('<VAR>')
                else:
                    encoded.append('<UNO>')

        # Объединение повторяющихся меток
        return self._merge_continuous(encoded)

    def _merge_continuous(self, encoded):
        """Объединение повторяющихся меток и обработка структуры \VAR"""
        result = []
        prev = None
        prev_prev = None  # Новое: хранение предыдущего предыдущего токена

        for i, item in enumerate(encoded):
            # Обработка повторяющихся меток (NUM/VAR)
            if item in ('<NUM>', '<VAR>') and item == prev:
                continue

            # Обработка структуры \VAR (если текущий VAR, а предыдущий — это \)
            if (item == '<VAR>' and prev == '\\' and
                str(encoded[i-1]) in self.latex_to_id.values()):  # Убедиться, что \ — отдельный токен
                result.pop()  # Удалить \
                result.append('<UNO>')
            else:
                result.append(item)

            prev_prev = prev
            prev = item

        return result

    def decode(self, encoded_ids):
        """
        Метод декодирования: преобразует список ID обратно в строку
        :param encoded_ids: Список закодированных ID
        :return: Декодированная строка
        """
        decoded_tokens = []
        for item in encoded_ids:
            if item == '<NUM>':
                decoded_tokens.append('0')
            elif item == '<VAR>':
                decoded_tokens.append('a')
            elif item == '<UNO>':
                decoded_tokens.append('1')
            else:
                # Обработка обычных ID
                try:
                    token_id = int(item)
                    decoded_tokens.append(self.id_to_latex.get(token_id, ''))
                except ValueError:
                    pass  # Пропуск невалидного ID
        return ''.join(decoded_tokens)

    def export_token_dict(self):
        """
        Создание сокращённого словаря токенов в формате DataFrame
        Возвращает DataFrame со столбцами:
        - token_id: Назначенный ID
        - latex: LaTeX-команда
        - category: Категория
        """
        dict_list = []
        for latex, token_id in self.latex_to_id.items():
            dict_list.append({
                "token_id": token_id,
                "latex": latex,
                "category": self.latex_category_map[latex]
            })

        return pd.DataFrame(dict_list).sort_values('token_id').reset_index(drop=True)


# Пример использования
if __name__ == "__main__":
    tokenizer = LatexTokenizer()

    test_case = r"\frac{(-5) \cdot \frac{1}{4}}{3 \cdot 1/5} = -\frac{5}{4} \cdot 5/3"  # Исходная строка
    tokens = tokenizer.tokenize(test_case)
    encoded = tokenizer.encode(test_case)
    decode = tokenizer.decode(encoded)

    print("Исходная строка:", test_case)
    print("Результат токенизации:", tokens)
    print("Результат кодирования:", encoded)
    print("Результат декодирования:", decode)
